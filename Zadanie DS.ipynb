{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEMANTIVE: DATA SCIENTIST RECRUITMENT TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please download and load the  [abalone dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/).  \n",
    "\n",
    "##### You can use information from [this](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names) file to add the proper headers to the columns. \n",
    "\n",
    "The whole task will be driven by supervised learning problem.\n",
    "Let's define target variable as $Rings / 1.5$ (it rougly corresponds to abalone's age).  \n",
    "We strongly encourage you use scikit-learn for the modeling tasks (but feel free to use different tools if you think they are appropriate).  \n",
    "\n",
    "##### First 2 tasks are obligatory. From the tasks 3-5 you can pick and complete 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Data exploration\n",
    "\n",
    "##### Explore the data  and provide a short summary of your findings. Pay attention to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Provided data contains 8 attributes and 1 target value that can be considered as both classification or regression\n",
    "    problem. It represents the rings of abalone that will be translated into its age. Depending on what model we will\n",
    "    be using it can be represented as it is or can be converted into dummy variables (what I will do further). Because \n",
    "    of the fact there are a lot of classes (from 1 to 29) accuracy of the classification models won't be very precise\n",
    "    and I'd would do a clustering first - dividing data into groups that will represent classes. \n",
    "    All the attributes except the Sex are numerical and need scaling. At the first glance data contain the \n",
    "    outliers so I did data detection additionally. Polynomial attribute Sex needs to be converted into binomial \n",
    "    dummy variables.\n",
    "    Before all models I am using I did the dimensionally reduction PCA or KernelPCA (one of them should be commented\n",
    "    as we shouldn't use both)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Supervised learning\n",
    "\n",
    "##### Prepare the data for the modeling.  \n",
    "\n",
    "###### Choose 2 supervised learning algorithms, that you think are suitable for this problem. Describe shortly how these algorithms work. Use them on the data and describe the results.  \n",
    "  \n",
    "Note. that we're more interested in comprehensive description and explanation of your choice than in model scores, so we don't expect you to tune your model yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the data from the file\n",
    "data = pd.read_csv('abalone.data', header=None)\n",
    "header = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight',\n",
    "          'Rings']\n",
    "data.columns = header\n",
    "\n",
    "# Splitting data into attributes and target value\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values\n",
    "\n",
    "# Converting Sex attribute into classes\n",
    "label_encoder = LabelEncoder()\n",
    "X[:, 0] = label_encoder.fit_transform(X[:, 0])\n",
    "# Converting Sex classes into dummy variables\n",
    "one_hot_encoder = OneHotEncoder(categorical_features=[0])\n",
    "X = one_hot_encoder.fit_transform(X).toarray()\n",
    "\n",
    "'''Outliers Detection'''\n",
    "clf = IsolationForest(max_samples=100, random_state=np.random.RandomState(42))\n",
    "clf.fit(X)\n",
    "outlier = clf.predict(X)\n",
    "X_test = X[outlier == 1]\n",
    "Y_test = Y[outlier == 1]\n",
    "\n",
    "'''Data standardization'''\n",
    "sc = StandardScaler()\n",
    "X[:, 2:] = sc.fit_transform(X[:, 2:])\n",
    "\n",
    "'''Dimension reduction algorithms provide us find strong correlations between attributes and use for our models\n",
    "    only that attributes which changes will affect our target value\n",
    "    The ones that we won't use have a slight influence so we can remove them from our dataset'''\n",
    "\n",
    "'''PCA dimension reduction: finding the strongest linear correlation between attribute and target value'''\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "explained_variance = pca.explained_variance_ratio_  # is used to select right number of components\n",
    "\n",
    "'''Kernel PCA dimension reduction: finding the strongest non-linear correlation between attribute and target value'''\n",
    "# kpca = KernelPCA(n_components=3, kernel='rbf')\n",
    "# X = kpca.fit_transform(X)\n",
    "# explained_variance = np.var(X, axis=0)\n",
    "# is used to select right number of components\n",
    "# explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "\n",
    "# Splitting dataset into training and test sets with 80/20 relation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "'''Random Tree Classification\n",
    "    Model is based on creating more than one decision tree (n_estimators)\n",
    "    Then it creates each decision tree from the random chosen data (k subset of our data)\n",
    "    When new attributes come each decision tree determine the class it belongs to. Majority of votes wins'''\n",
    "rfr = RandomForestClassifier(n_estimators=64)\n",
    "rfr.fit(X_train, Y_train)\n",
    "rfr_predictions = rfr.predict(X_test)\n",
    "rfr_predictions = np.reshape(rfr_predictions, (-1, 1))\n",
    "\n",
    "accuracy_score(Y_test, rfr_predictions)\n",
    "accuracies_rfc = cross_val_score(estimator=rfr, X=X_train, y=Y_train, cv=10)\n",
    "\n",
    "'''Artificial Neural Network Classification\n",
    "    Neural Networks are made of neurons that have inputs and activation functions and of the layers of this neurons\n",
    "    that are connected between each other. Typically ann is made of input, output and hidden layers. Output layers\n",
    "    are learned from the output they should give (our training set) - they compare the output they get and the output\n",
    "    they should have and compare them. The error is delivered to the connected neurons so the weights they have\n",
    "    are changing - called backpropagation'''\n",
    "\n",
    "# Converting target value into dummy variables for ANN classification\n",
    "one_hot_encoder = OneHotEncoder(categorical_features=[0])\n",
    "Y_nn = one_hot_encoder.fit_transform(np.reshape(Y, (-1, 1))).toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_nn, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# The input layer changes depending which dimensionally reduction function we are using\n",
    "# 3 in case KernelPCA and 2 in case PCA. \n",
    "# classifier.add(Dense(units=18, kernel_initializer='uniform', activation='relu', input_dim=3))\n",
    "classifier.add(Dense(units=18, kernel_initializer='uniform', activation='relu', input_dim=2))\n",
    "classifier.add(Dense(units=11, kernel_initializer='uniform', activation='softmax'))\n",
    "classifier.add(Dense(units=7, kernel_initializer='uniform', activation='relu'))\n",
    "classifier.add(Dense(units=28, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, Y_train, epochs=150)\n",
    "\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "predictions = np.array([[1.0 if j == i.max() else 0.0 for j in i] for i in y_pred])\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Other tasks - Pick 2 of them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Dimensionality reduction\n",
    "\n",
    "\n",
    "##### What are the applications of the dimensionality reduction?   \n",
    "\n",
    "###### Pick two algorithms that could be useful for exploration or supervised learning problem and apply them on the data. Describe what you've found. Provide a short description of the algorithms you've chosen.\n",
    "\n",
    "Note: feature selection is also one type of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Dimension reduction algorithms provide us find strong correlations between attributes and use for our models\n",
    "    only that attributes which changes will affect our target value\n",
    "    The ones that we won't use have a slight influence so we can remove them from our dataset'''\n",
    "\n",
    "'''PCA dimension reduction: finding the strongest linear correlation between attribute and target value'''\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "explained_variance = pca.explained_variance_ratio_  # is used to select right number of components\n",
    "\n",
    "'''Kernel PCA dimension reduction: finding the strongest non-linear correlation between attribute and target value'''\n",
    "# kpca = KernelPCA(n_components=3, kernel='rbf')\n",
    "# X = kpca.fit_transform(X)\n",
    "# explained_variance = np.var(X, axis=0)\n",
    "# is used to select right number of components\n",
    "# explained_variance_ratio = explained_variance / np.sum(explained_variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Clustering\n",
    "\n",
    "##### What is clustering used for? Name the popular types of clustering. Pick two clustering algorithms and run them on the dataset. Describe what you've found. Does it help with the supervised learning task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clustering \n",
    "    Clustering is used to split similar data into the groups which can be used for class reduction that can\n",
    "    increase the accuracy of classification models\n",
    "    Clustering is the type of unsupervised learning - no need of test set'''\n",
    "'''KMeans Clustering\n",
    "    This algorithm choose a n_clusters centroid (they are chosen by k means ++ algorithm which preserve us\n",
    "    from the random initialization trap). Then we assign all the points we have to this centroids. After \n",
    "    we change their coordinates on the average sum of the assigned coordinates. Repeating this steps until\n",
    "    the points stop reassigning\n",
    "    To find the optimal number of clusters we are using within cluster sum of squares (The sum of the squared deviations\n",
    "     from each observation and the cluster centroid.)'''\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plot.plot(range(1, 11), wcss)\n",
    "plot.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "y_means = kmeans.fit_predict(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y_means, test_size=0.2, random_state=0)\n",
    "\n",
    "'''Using some other classification models to prove that accuracy of prediction clustered target value is improved'''\n",
    "\n",
    "'''Random Forest Classification'''\n",
    "rfc = RandomForestClassifier(n_estimators=64)\n",
    "rfc.fit(X_train, Y_train)\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, rfc_pred)\n",
    "\n",
    "'''KNN Classification '''\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, Y_train)\n",
    "knn_prediction = knn_classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, knn_prediction)\n",
    "\n",
    "'''SVC Classification'''\n",
    "svc_classifier = SVC(C=2.0)\n",
    "svc_classifier.fit(X_train, Y_train)\n",
    "svc_prediction = svc_classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, svc_prediction)\n",
    "\n",
    "'''k-fold cross validation is based on splitting a dataset into cv numbers of folds, 1 of them is using for testing\n",
    "    9 of them is for training. And we are continuing until each of the folds is used for testing our model'''\n",
    "'''k-fold cross validation'''\n",
    "accuracies_rfc = cross_val_score(estimator=rfc, X=X, y=y_means, cv=10)\n",
    "accuracies_knn = cross_val_score(estimator=knn_classifier, X=X, y=y_means, cv=10)\n",
    "accuracies_svc = cross_val_score(estimator=svc_classifier, X=X, y=y_means, cv=10)\n",
    "\n",
    "\n",
    "'''Hierarchical Clustering'''\n",
    "'''This algorithm based on making dendogram. First we are considering each point as a separate cluster. Then we  \n",
    "    union the closest clusters into one and continuing until there is one cluster left. \n",
    "    To find the optimal number of clusters we should look at dendogram we made and choose the longest vertical line \n",
    "    that isn't crossing any horizontal. Than draw a horizontal perpendicular line at the middle - number of crossed \n",
    "    lines is a number of clusters we should have'''\n",
    "dendogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
    "plot.title('Dendogram')\n",
    "plot.show()\n",
    "\n",
    "hierc = AgglomerativeClustering(n_clusters=2)\n",
    "y_cl = hierc.fit_predict(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y_cl, test_size=0.2, random_state=0)\n",
    "\n",
    "'''Random Forest Classification'''\n",
    "rfc = RandomForestClassifier(n_estimators=64)\n",
    "rfc.fit(X_train, Y_train)\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, rfc_pred)\n",
    "\n",
    "'''KNN Classification '''\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, Y_train)\n",
    "knn_prediction = knn_classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, knn_prediction)\n",
    "\n",
    "'''SVC Classification'''\n",
    "svc_classifier = SVC(C=2.0)\n",
    "svc_classifier.fit(X_train, Y_train)\n",
    "svc_prediction = svc_classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, svc_prediction)\n",
    "\n",
    "'''k-fold cross validation'''\n",
    "accuracies_rfc_hierarchical = cross_val_score(estimator=rfc, X=X, y=y_cl, cv=10)\n",
    "accuracies_knn_hierarchical = cross_val_score(estimator=knn_classifier, X=X, y=y_cl, cv=10)\n",
    "accuracies_svc_hierarchical = cross_val_score(estimator=svc_classifier, X=X, y=y_cl, cv=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Hyperparameter selection and crossvalidation\n",
    "\n",
    "##### We imagine that you did some modeling in 2nd task with the methods that have some tunable hyperparameters. If they don't, either find a versions of them that have that are tunable, or pick the different ones.\n",
    "\n",
    "##### Tune the hyperparameters of your model using cross-validation. Does it make it better? Does it solve overfitting problems? Is cross-validation score worse than score that your model achieves on test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''k-fold cross validation is based on splitting a dataset into cv numbers of folds, 1 of them is using for testing\n",
    "    9 of them is for training. And we are continuing until each of the folds is used for testing our model'''\n",
    "'''k-fold cross validation'''\n",
    "accuracies_rfc_hierarchical = cross_val_score(estimator=rfc, X=X, y=y_cl, cv=10)\n",
    "accuracies_knn_hierarchical = cross_val_score(estimator=knn_classifier, X=X, y=y_cl, cv=10)\n",
    "accuracies_svc_hierarchical = cross_val_score(estimator=svc_classifier, X=X, y=y_cl, cv=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
